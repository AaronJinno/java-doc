---
title: 5-6. 文档分析
date: 2023-06-28
---
:::tip 文档分析是干嘛的
将一句话（段落）进行分析，拆解成N个关键字，然后使用关键字进行索引。
- 存储：对储存的文本进行拆解，形成几个关键字（比如视频的描述信息）
- 搜索：对用户搜索的句子进行插件，形成几个关键字，与ES的关键字比对。

文档分析就是对句子（段落）拆解与标准化的 操作。
:::

## 文档分析介绍
#### 1. 为什么需要文档分析
用户搜索的时候，很大概率输入的是一段话，将一段话进行匹配是不精准也艰难的。

正确的做法就是提取这句话的关键信息：关键字。

然后使用关键字与ES中文档保存的字段的值做匹配，从而获取搜索的内容。

#### 2. 文档分析包括什么
文档分析包含下面两个过程
- 将一段`文本` 分成适合于 `倒排索引` 的独立的 `词条`
- 将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall(返回率)


## 分析器
#### 1. 分析器介绍
分析器就是用来做文档分析的，他一共包含了三个功能：
- 字符过滤器：字符串按顺序通过每个字符过滤器。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉 HTML，或者将 & 转化成 and等。
- 分词器：字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。
- Token 过滤器：一种标准化的行为，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。


#### 2. 内置分析器
Elasticsearch 还附带了可以直接使用的预包装的分析器。

接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条："Set the shape to semi-transparent by calling set_trans(5)"

##### 2.1 标准分析器
标准分析器是 Elasticsearch 默认使用的分析器。它是分析各种语言文本最常用的选择。
它根据 Unicode 联盟定义的 **单词边界** 划分文本。并删除绝大部分标点。最后，将词条小写。

分析结果：set, the, shape, to, semi, transparent, by, calling, set_trans, 5

##### 2.2 简单分析器
简单分析器在任何不是字母的地方分隔文本，将词条小写。

分析结果：set, the, shape, to, semi, transparent, by, calling, set, trans

##### 2.3 空格分析器
空格分析器在空格的地方划分文本。

分析结果：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)


##### 2.4 语言分析器
特定语言分析器可用于很多语言。它们可以考虑指定语言的特点。

例如， 英语分析器附带了一组英语无用词（例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。

分析结果：set, shape, semi, transpar, call, set_tran, 5 （transparent、 calling 和 set_trans 已经变为词根格式）


#### 3. 分析器使用场景
当我们索引一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域搜索的时候，我们需要将查询字符串通过相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致

- 当你查询一个全文域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。
- 当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。


## 使用分析器

#### 1. analyze API
analyze API 可以用来查看文本是如何被分析器分析的。该API只是为了方便理解分词的过程和实际被存储到索引中的词条是什么。

#### 2. 使用analyze
基本用法如下：

请求： GET http://localhost:9200/_analyze

请求体：
```json
{
    "analyzer": "standard", //指定一个分析器，这里选择standard
    "text": "Text to anlyze" //待测试的文本
}
```

分析的结果：
```json
{
    "tokens": [
        {
            "token": "text",
            "start_offset": 0,
            "end_offset": 4,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "to",
            "start_offset": 5,
            "end_offset": 7,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "anlyze",
            "start_offset": 8,
            "end_offset": 14,
            "type": "<ALPHANUM>",
            "position": 2
        }
    ]
}
```
如上：token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。start_offset 和 end_offset 指明字符在原始字符串中的位置。

#### 3. 指定分析器
- 指定适合的分析器：可以根据语言类型和使用场景指定适合的分析器，而不总是使用standard
- 不使用分析器：有些场景是不进行分词的，比如匹配用户的ID 或者一个标签。
    - 此时无需使用分析器，需要手动设置映射


## 中文处理
内置的分词器对中文的处理有些问题，我们需要手动加载专门处理中文的分词器：IK
#### 1. 中文的默认处理
如下，尝试对中文进行分词处理。

请求：GET http://localhost:9200/_analyze
请求体：
```json
{
    //采用默认的分词器
    "text": "测试单词" //待测试的文本
}

分词结果：
```json
{
    "tokens": [
        {
            "token": "测",
            "start_offset": 0,
            "end_offset": 1,
            "type": "<IDEOGRAPHIC>",
            "position": 0
        },
        {
            "token": "试",
            "start_offset": 1,
            "end_offset": 2,
            "type": "<IDEOGRAPHIC>",
            "position": 1
        },
        {
            "token": "单",
            "start_offset": 2,
            "end_offset": 3,
            "type": "<IDEOGRAPHIC>",
            "position": 2
        },
        {
            "token": "词",
            "start_offset": 3,
            "end_offset": 4,
            "type": "<IDEOGRAPHIC>",
            "position": 3
        }
    ]
}
```

ES 的默认分词器无法识别中文的词语，只能简单的将每个字拆开，这基本毫无意义，我们预期的结果是根据词语分词，比如上述测试的预期结果应该是：“测试”，“单词”

#### 2. IK 分词器
ES没有内置的支持中文的分词器，因此我们需要手动下载一个中文分词器，然后以ES插件的形式使用。

常用的中文分词器为：IK中文分词器

IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包，特性如下：
- 采用了特有的“正向迭代最细粒度切分算法“，具有60万字/秒的高速处理能力
- 采用了多子处理器分析模式，支持：英文字母（IP地址、Email、URL）、数字（日期，常用中文数量词，罗马数字，科学计数法），中文词汇（姓名、地名处理）等分词处理。
- 对中英联合支持不是很好,在这方面的处理比较麻烦，需再做一次查询,同时是支持个人词条的优化的词典存储，更小的内存占用。
- 支持用户词典扩展定义。
- 针对Lucene全文检索优化的查询分析IKQueryParser；采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。




#### 3. 安装 IK 分词器
- 下载地址：[github-releases](https://github.com/medcl/elasticsearch-analysis-ik/releases)
- 版本：IK分词器的版本要与ElasticSearch的版本完全对应。
    - 否则会报错：Plugin [ analysis-ik-8.7.0 ] is missing a descriptor properties file.
- 安装：绿色软件，解压后的文件放入ES根目录下的plugins目录内。

重启ES后 IK分词器 就生效了。

#### 4. 使用IK分词器

## 自定义分析器